<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Terminal</title>
    <link>//localhost:1313/</link>
    <description>Recent content on Terminal</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Mon, 24 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Run a Powerful Offline AI Model from a USB Stick (Windows Guide)</title>
      <link>//localhost:1313/posts/run-a-powerful-offline-ai-model-from-a-usb-stick-windows-guide/</link>
      <pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/run-a-powerful-offline-ai-model-from-a-usb-stick-windows-guide/</guid>
      <description>&lt;p&gt;What if you could carry the equivalent of &lt;strong&gt;127 million novels&lt;/strong&gt; or &lt;strong&gt;the entire Wikipedia 2,500 times over&lt;/strong&gt;&amp;hellip; in your pocket?&#xA;With the &lt;strong&gt;Dolphin-LLaMA3&lt;/strong&gt; model from Hugging Face (via Ollama), you can. This model fits on any &lt;strong&gt;128GB USB drive&lt;/strong&gt;, taking up just &lt;strong&gt;10GB&lt;/strong&gt; of space, and runs &lt;strong&gt;fully offline&lt;/strong&gt; â€” completely detached from Big Tech servers, censorship filters, or surveillance.&lt;/p&gt;&#xA;&lt;h1 id=&#34;contents&#34;&gt;Contents&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;About the Model&lt;/li&gt;&#xA;&lt;li&gt;Routine Overview&lt;/li&gt;&#xA;&lt;li&gt;Initial Setup on Windows&lt;/li&gt;&#xA;&lt;li&gt;Running from a USB Drive&lt;/li&gt;&#xA;&lt;li&gt;Running AI from the USB&lt;/li&gt;&#xA;&lt;li&gt;Improve the Interface with AnythingLLM&lt;/li&gt;&#xA;&lt;li&gt;Interacting with Dolphin via Python (API)&lt;/li&gt;&#xA;&lt;li&gt;Final Thoughts&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;-about-the-model&#34;&gt;ðŸ§  About the Model&lt;/h2&gt;&#xA;&lt;p&gt;Weâ€™ll be using the &lt;strong&gt;Dolphin-LLaMA3&lt;/strong&gt; model, available directly through &lt;a href=&#34;https://ollama.com/library/dolphin-llama3&#34;&gt;Ollama&lt;/a&gt;. This 8-billion parameter LLaMA3-based model was trained on &lt;strong&gt;15 trillion tokens&lt;/strong&gt;, equivalent to about &lt;strong&gt;60 terabytes&lt;/strong&gt; of text.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An Obsidian IDE for Python</title>
      <link>//localhost:1313/posts/an-obsidian-ide-for-python/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/an-obsidian-ide-for-python/</guid>
      <description>&lt;h2 id=&#34;you-did-what&#34;&gt;You did what?!&lt;/h2&gt;&#xA;&lt;p&gt;Let me explain: You write your notes in your Vault. Then, you run a command, and the code inside becomes a script and executes! And it does so with the environment of your choice already activated.&lt;/p&gt;&#xA;&lt;h2 id=&#34;oh-i-see-now-this-is-a-notebook&#34;&gt;Oh, I see now. This is a notebook!&lt;/h2&gt;&#xA;&lt;p&gt;No, itâ€™s not. It transforms every code blockâ€”including commentsâ€”into an independent &lt;code&gt;script.py&lt;/code&gt; inside your &lt;code&gt;.obsidian\scripts\python&lt;/code&gt; folder. From there, you can grab it and start deploying or packaging it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running a Local AI to Interact with Your Vault</title>
      <link>//localhost:1313/posts/running-a-local-ai-to-interact-with-your-vault/</link>
      <pubDate>Wed, 20 Dec 2023 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/running-a-local-ai-to-interact-with-your-vault/</guid>
      <description>&lt;h2 id=&#34;from-ollama-installation-to-ai-integration&#34;&gt;From: Ollama Installation. To: AI Integration&lt;/h2&gt;&#xA;&lt;p&gt;Inspired once more by &lt;a href=&#34;https://www.youtube.com/watch?v=Wjrdr0NU4Sk&#34;&gt;NetworkChuck&lt;/a&gt;, this guide walks you through setting up Ollama on your system, integrating various AI models, and enhancing your workflow with ==Stable Diffusion== and ==BMO== or ==Smart Connections== on obsidian. Whether you&amp;rsquo;re a developer or an AI enthusiast, this step-by-step tutorial will help you harness the power of local AI models effectively.&lt;/p&gt;&#xA;&lt;p&gt;If you also enjoy configuring your AI environment and exploring different models, this approach will provide you with the control and flexibility you need!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building a Blog System Using Obsidian</title>
      <link>//localhost:1313/posts/building-a-blog-system-using-obsidian/</link>
      <pubDate>Wed, 13 Dec 2023 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/building-a-blog-system-using-obsidian/</guid>
      <description>&lt;h2 id=&#34;from-vaults-folder-to-blogs-feed&#34;&gt;From: Vault&amp;rsquo;s folder. To: Blog&amp;rsquo;s feed&lt;/h2&gt;&#xA;&lt;p&gt;This is my personal blogging pipeline, inspired by the content creator &lt;a href=&#34;https://www.youtube.com/watch?v=dnE7c0ELEH8&amp;amp;list=PLazumvohNo-2qvB49-9RL4karLMmqBDas&amp;amp;index=18&#34;&gt;NetWorkChuck&lt;/a&gt;. I hope this guide helps you express your ideas more effectively and share them with the world.&lt;/p&gt;&#xA;&lt;p&gt;If you enjoy maintaining control over your publications and having some code-based automation fun, this approach should serve you well!&lt;/p&gt;&#xA;&lt;p&gt;Whatâ€™s happening here is that an Obsidian vault is feeding the content of the posts youâ€™re reading right now. Once everything is set up, the process requires minimal effort to keep running smoothly.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
